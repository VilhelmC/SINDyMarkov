{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import os\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Check if results exist\n",
    "def load_results(filepath):\n",
    "    \"\"\"Load results from CSV file if it exists.\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        return pd.read_csv(filepath)\n",
    "    else:\n",
    "        print(f\"Warning: {filepath} not found.\")\n",
    "        return None\n",
    "\n",
    "# Load all experiment results\n",
    "simple_results = load_results('results/simple_example_results.csv')\n",
    "lambda_sigma_results = load_results('results/lambda_sigma_experiment_results.csv')\n",
    "multiterm_results = load_results('results/multiterm_experiment_results.csv')\n",
    "\n",
    "# Function to analyze individual experiment results\n",
    "def analyze_experiment(results, title):\n",
    "    \"\"\"Analyze and visualize results from a single experiment.\"\"\"\n",
    "    if results is None:\n",
    "        print(f\"No data available for: {title}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\"*len(title))\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Number of data points: {len(results)}\")\n",
    "    if 'discriminability' in results.columns:\n",
    "        print(f\"Discriminability range: {results['discriminability'].min():.2f} to {results['discriminability'].max():.2f}\")\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    r2 = r2_score(results['empirical_prob'], results['theoretical_prob'])\n",
    "    rmse = np.sqrt(mean_squared_error(results['empirical_prob'], results['theoretical_prob']))\n",
    "    bias = np.mean(results['theoretical_prob'] - results['empirical_prob'])\n",
    "    \n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"Bias: {bias:.4f}\")\n",
    "    \n",
    "    # Create plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Plot 1: Discriminability vs Success Probability\n",
    "    if 'discriminability' in results.columns:\n",
    "        ax1.scatter(results['discriminability'], results['empirical_prob'], \n",
    "                   label='Empirical', alpha=0.7, s=80, color='blue')\n",
    "        \n",
    "        # Sort for smoother line\n",
    "        sorted_idx = results['discriminability'].argsort()\n",
    "        ax1.plot(results['discriminability'].iloc[sorted_idx], \n",
    "                results['theoretical_prob'].iloc[sorted_idx], \n",
    "                label='Theoretical', linewidth=2, color='red')\n",
    "        \n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_xlabel('Discriminability (D)')\n",
    "        ax1.set_ylabel('Success Probability')\n",
    "        ax1.set_title('Success Probability vs Discriminability')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, \"Discriminability data not available\", \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        ax1.axis('off')\n",
    "    \n",
    "    # Plot 2: Direct Comparison\n",
    "    ax2.scatter(results['theoretical_prob'], results['empirical_prob'], \n",
    "               alpha=0.7, s=80)\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    min_val = min(results['theoretical_prob'].min(), results['empirical_prob'].min())\n",
    "    max_val = max(results['theoretical_prob'].max(), results['empirical_prob'].max())\n",
    "    ax2.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 Line')\n",
    "    \n",
    "    # Add metrics to plot\n",
    "    ax2.text(0.05, 0.95, f'R² = {r2:.4f}\\nRMSE = {rmse:.4f}\\nBias = {bias:.4f}',\n",
    "            transform=ax2.transAxes, fontsize=12,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax2.set_xlabel('Theoretical Success Probability')\n",
    "    ax2.set_ylabel('Empirical Success Probability')\n",
    "    ax2.set_title('Theoretical vs Empirical Comparison')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional analysis based on experiment parameters\n",
    "    if 'n_samples' in results.columns and 'data_range' in results.columns:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pivot = results.pivot_table(\n",
    "            index='data_range',\n",
    "            columns='n_samples',\n",
    "            values=['empirical_prob', 'theoretical_prob']\n",
    "        )\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = results['empirical_prob'] - results['theoretical_prob']\n",
    "        \n",
    "        # Plot residuals vs discriminability\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if 'discriminability' in results.columns:\n",
    "            plt.scatter(results['discriminability'], residuals, alpha=0.7, s=70)\n",
    "            plt.axhline(y=0, color='r', linestyle='--')\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('Discriminability (D)')\n",
    "            plt.ylabel('Residual (Empirical - Theoretical)')\n",
    "            plt.title('Residual Analysis')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "# Function to analyze lambda/sigma ratio experiment\n",
    "def analyze_lambda_sigma_experiment(results):\n",
    "    \"\"\"Analyze the effect of lambda/sigma ratio on model performance.\"\"\"\n",
    "    if results is None:\n",
    "        print(\"No lambda/sigma experiment data available.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nLambda/Sigma Ratio Experiment Analysis\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # Get unique ratios\n",
    "    ratios = sorted(results['lambda_sigma_ratio'].unique())\n",
    "    print(f\"Lambda/Sigma ratios tested: {ratios}\")\n",
    "    \n",
    "    # Calculate metrics for each ratio\n",
    "    metrics_by_ratio = []\n",
    "    for ratio in ratios:\n",
    "        ratio_results = results[results['lambda_sigma_ratio'] == ratio]\n",
    "        r2 = r2_score(ratio_results['empirical_prob'], ratio_results['theoretical_prob'])\n",
    "        rmse = np.sqrt(mean_squared_error(ratio_results['empirical_prob'], ratio_results['theoretical_prob']))\n",
    "        bias = np.mean(ratio_results['theoretical_prob'] - ratio_results['empirical_prob'])\n",
    "        \n",
    "        metrics_by_ratio.append({\n",
    "            'lambda_sigma_ratio': ratio,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'bias': bias,\n",
    "            'n_samples': len(ratio_results)\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_by_ratio)\n",
    "    print(metrics_df)\n",
    "    \n",
    "    # Plot metrics vs ratio\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    ax1.plot(metrics_df['lambda_sigma_ratio'], metrics_df['r2'], marker='o', linewidth=2)\n",
    "    ax1.set_xlabel('λ/σ Ratio')\n",
    "    ax1.set_ylabel('R² Score')\n",
    "    ax1.set_title('R² vs λ/σ Ratio')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(metrics_df['lambda_sigma_ratio'], metrics_df['rmse'], marker='o', linewidth=2, color='red')\n",
    "    ax2.set_xlabel('λ/σ Ratio')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax2.set_title('RMSE vs λ/σ Ratio')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3.plot(metrics_df['lambda_sigma_ratio'], metrics_df['bias'], marker='o', linewidth=2, color='green')\n",
    "    ax3.set_xlabel('λ/σ Ratio')\n",
    "    ax3.set_ylabel('Bias')\n",
    "    ax3.set_title('Bias vs λ/σ Ratio')\n",
    "    ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot success probability curves for different ratios\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for ratio in ratios:\n",
    "        ratio_results = results[results['lambda_sigma_ratio'] == ratio]\n",
    "        # Sort for smoother line\n",
    "        sorted_idx = ratio_results['discriminability'].argsort()\n",
    "        \n",
    "        plt.scatter(ratio_results['discriminability'], ratio_results['empirical_prob'], \n",
    "                   label=f'Empirical λ/σ={ratio}', alpha=0.5, s=60)\n",
    "        plt.plot(ratio_results['discriminability'].iloc[sorted_idx], \n",
    "                ratio_results['theoretical_prob'].iloc[sorted_idx], \n",
    "                label=f'Theory λ/σ={ratio}', linestyle='--')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Discriminability (D)')\n",
    "    plt.ylabel('Success Probability')\n",
    "    plt.title('Effect of λ/σ Ratio on Success Probability')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot phase diagram with color based on ratio\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(results['discriminability'], results['empirical_prob'], \n",
    "                         c=results['lambda_sigma_ratio'], cmap='viridis', \n",
    "                         s=80, alpha=0.7)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('λ/σ Ratio')\n",
    "    \n",
    "    # Plot theoretical curves for each ratio\n",
    "    for ratio in ratios:\n",
    "        ratio_results = results[results['lambda_sigma_ratio'] == ratio]\n",
    "        # Sort for smoother line\n",
    "        sorted_idx = ratio_results['discriminability'].argsort()\n",
    "        plt.plot(ratio_results['discriminability'].iloc[sorted_idx], \n",
    "                ratio_results['theoretical_prob'].iloc[sorted_idx], \n",
    "                'k--', alpha=0.5)\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Discriminability (D)')\n",
    "    plt.ylabel('Success Probability')\n",
    "    plt.title('Phase Diagram with λ/σ Ratio')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Function to compare all experiments\n",
    "def compare_all_experiments():\n",
    "    \"\"\"Compare results across all experiments.\"\"\"\n",
    "    if simple_results is None or multiterm_results is None:\n",
    "        print(\"Not enough data to compare experiments.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nComparison Across Experiments\")\n",
    "    print(\"============================\")\n",
    "    \n",
    "    # Combine results from different experiments\n",
    "    simple_results_copy = simple_results.copy()\n",
    "    simple_results_copy['experiment'] = 'Simple (1 true term)'\n",
    "    \n",
    "    multiterm_results_copy = multiterm_results.copy()\n",
    "    multiterm_results_copy['experiment'] = 'Multiple (2 true terms)'\n",
    "    \n",
    "    combined = pd.concat([simple_results_copy, multiterm_results_copy], ignore_index=True)\n",
    "    \n",
    "    # Summary metrics\n",
    "    metrics_by_exp = []\n",
    "    for exp in combined['experiment'].unique():\n",
    "        exp_results = combined[combined['experiment'] == exp]\n",
    "        r2 = r2_score(exp_results['empirical_prob'], exp_results['theoretical_prob'])\n",
    "        rmse = np.sqrt(mean_squared_error(exp_results['empirical_prob'], exp_results['theoretical_prob']))\n",
    "        \n",
    "        metrics_by_exp.append({\n",
    "            'experiment': exp,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'n_samples': len(exp_results)\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_by_exp)\n",
    "    print(metrics_df)\n",
    "    \n",
    "    # Plot combined results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for exp in combined['experiment'].unique():\n",
    "        exp_results = combined[combined['experiment'] == exp]\n",
    "        \n",
    "        plt.scatter(exp_results['discriminability'], exp_results['empirical_prob'], \n",
    "                   label=f'{exp} (Empirical)', alpha=0.7, s=70)\n",
    "        \n",
    "        # Sort for smoother line\n",
    "        sorted_idx = exp_results['discriminability'].argsort()\n",
    "        plt.plot(exp_results['discriminability'].iloc[sorted_idx], \n",
    "                exp_results['theoretical_prob'].iloc[sorted_idx], \n",
    "                label=f'{exp} (Theory)')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Discriminability (D)')\n",
    "    plt.ylabel('Success Probability')\n",
    "    plt.title('Success Probability vs Discriminability Across Experiments')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Direct comparison plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for exp in combined['experiment'].unique():\n",
    "        exp_results = combined[combined['experiment'] == exp]\n",
    "        \n",
    "        plt.scatter(exp_results['theoretical_prob'], exp_results['empirical_prob'], \n",
    "                   label=exp, alpha=0.7, s=70)\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='1:1 Line')\n",
    "    \n",
    "    plt.xlabel('Theoretical Success Probability')\n",
    "    plt.ylabel('Empirical Success Probability')\n",
    "    plt.title('Theory vs Empirical Across Experiments')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Run the analyses if the data exists\n",
    "if simple_results is not None:\n",
    "    analyze_experiment(simple_results, \"Simple Three-Term Example\")\n",
    "\n",
    "if multiterm_results is not None:\n",
    "    analyze_experiment(multiterm_results, \"Multiple True Terms Experiment\")\n",
    "\n",
    "if lambda_sigma_results is not None:\n",
    "    analyze_lambda_sigma_experiment(lambda_sigma_results)\n",
    "\n",
    "# Compare all experiments\n",
    "if simple_results is not None and multiterm_results is not None:\n",
    "    compare_all_experiments()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
